---
title: "A simulation model of DNA methylation data"
author: "Peter Hickey"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    fig_caption: yes
    includes:
      in_header: ../latex/header.tex
  html_document:
    keep_md: true
bibliography: ../latex/phd_thesis.bib
---

\chapter{A simulation model of DNA methylation data}\label{chap:methsim}

\begin{chapabstract}
This chapter describes the \texttt{methsim} software to simulate DNA methylation data. \texttt{methsim} incorporates a model of within-fragment co-methylation and simulates individual sequencing reads, which sets it apart from existing software. We explore different parameter choices and highlight promising directions of research, such as using \texttt{methsim} to simulate data in order to compare methods for identifying differential methylation.
\end{chapabstract}

\section{Introduction}\label{sec:methsim_introduction}

It can be difficult to design an experiment that can be used for validating or benchmarking different analysis strategies. In fact, the huge variety of experimental factors, and their possible values, can make such a task infeasible. But perhaps the bigger hurdle is that there are not attractive experiments to perform; why spend your time and money on an experiment where you 'know the answer' when you could be spending that same time and money on investigating some new biology? In these scenarios and others, simulation studies play a vital role in applied statistics, where they can be used in the development, validation and benchmarking of different analysis methodologies.

The key advantage of simulated data is that we know the truth _a priori_. Moreover, we can manipulate the truth via parameters in the simulation model and can examine how a method performs under a variety of scenarios. And this manipulation is cheap, a mere matter of changing parameters and re-running a piece of software, which means that we can investigate a broad range of plausible scenarios.

Simulation studies can provide many insights into the performance of a method. At the most basic level, if a method fails performs poorly when applied to 'easy' simulated data, then it is very unlikely to work well when applied to more complex real data. We can also identify which scenarios are 'easy' (ones where most methods are able to identify the truth) and scenarios under which certain methods perform better than others. While simulated data can never fully capture the richness of real data, we can learn a lot about a method by studying how it performs when applied to simulated data.

Simulation methods may also be used to learn about the plausibility of hypothesised models of a phenomenon, be they mechanistic or stochastic. By simulating data from the proposed model, and comparing it to the real data, we can identify hypotheses that are incompatible with reality. This can also help identify shortcomings in the model so that it may be refined in an iterative manner.

There are a few key criteria when designing a simulation method:

1. Realism: The simulated data must be 'similar' to the real data. While this is obvious, it is also often hard to pin down or agree upon what constitutes 'similar enough'.
2. Cost: It should be fast and cheap to simulate data. The most common use of simulation models in applied statistics is to repeatedly generate datasets under a range of parameter settings. This requires that each simulation is fast and computationally cheap, otherwise it will be prohibitive to analyse the full space of scenarios. An exception to this rule may occur when the simulation model is used to test a proposed mechanistic or stochastic model of a phenomenon, such as in studies of molecular dynamics. Even then, however, the cost of a simulation should be less than the cost of performing the equivalent experiments, otherwise the simulation is generally not worth the effort.
3. Usability: There __must__ be a software implementation. Simulation models exist to be simulated from; a simulation model without an implementation is next to useless. The implementation should give the user easy access to the key  parameters and have sensible default settings. The output of the software should be in a standard format or readily convertible to a simple, manipulable format.

There is a lack of gold-standard datasets in the field of DNA methylation for the benchmarking and validation of analysis methods. Therefore, the development of realistic, cheap and user-friendly software to simulate DNA methylation data will be of benefit.

\section{Literature review}\label{sec:methsim_lit_review}

The simulation methods described here can be thought to lie on a spectrum with model-based methods at one end and re-sampling based methods at the other end[^model-based]. It is often simpler to simulate from a model-based simulation, particularly if the model is a well-studied parametric distribution. However, this simplicity often comes at the cost of increased assumptions, whose validity may be questionable. Simulations based on sampling of real data may reduce the number of assumptions required. However, care must be taken in selecting the units to be sampled so that the sampling process is efficient and so that the sampled data don't grossly distort within- and between-sample dependencies.

Any procedure for simulating DNA methylation data should obviously be tailored to its purpose. For example, if the study is comparing alignment software for bisulfite-sequencing data then the simulation software should produce `FASTQ` files with realistic base quality score profiles and sequencing errors. On the other hand, it may be sufficient to simulate some aggregate data, such as $\beta$-values, when the simulated data are to be used for comparing methods to identify differential methylation.

[^model-based]: Of course, the parameters in any model-based simulation should be based on real data, although this may not use a formal estimation procedure such as maximum likelihood.

\subsection{Methods for simulating bisulfite-sequencing reads}


All of the currently available methods for simulating bisulfite-sequencing reads are designed for the comparison of alignment strategies and are model-based. These are not suitable for comparisons of downstream analysis methods.

`Sherman` ([http://www.bioinformatics.babraham.ac.uk/projects/sherman/](http://www.bioinformatics.babraham.ac.uk/projects/sherman/)) is software to simulate bisulfite-sequencing reads, including various 'contaminants', such as SNPs, basecall errors and sequence artefacts as `FASTQ` files. The simulated reads are designed for comparing the performance of different alignment strategies. `Sherman` has many parameters, of which the ones relevant to our discussion of simulating realistic DNA methylation data are `-CG` and `-CH`, the bisulfite conversion rates for CG and CH methylation loci, respectively. These are set by the user with values between $0$ and $100$ ($\%$). Reads are simulated by sampling from the user-specified reference genome. When a read contains a CG (resp. CH) locus, it is randomly assigned as being converted (unmethylated) with probability `-CG` / 100 (resp. `-CH` / 100).  

While suitable for comparing alignment strategies, `Sherman` produces data that is not suitable for use in comparing downstream analysis methods. All CG (resp. CH) loci have an average $\beta$-value of `-CG` (resp. `-CG`) regardless of the genomic context, which we know to be incompatible with real data. Furthermore, the methylation state of each methylation locus is independent, which is clearly inconsistent with the strong co-methylation observed in real data.

`DNemulator` ([http://www.cbrc.jp/dnemulator/README.html](http://www.cbrc.jp/dnemulator/README.html)) uses a slightly more sophisticated simulation strategy to simulate `FASTQ` files for use in comparing alignment strategies for bisulfite-sequencing data. `DNemulator` does this with three separate routines, `fasta-methly-sim`, `fasta-polymorph` and `fasta-bisulf-sim`:

1. `fasta-methyl-sim` converts cytosines in the reference genome (`FASTA` file) to a character indicating the methylation level of that locus: `C` represents $0\%$ methylated, `c` represents $10\%$ methylated, `d` represents $20\%$ methylated, `v` represents $50\%$ methylated and `t` represents $100\%$ methylated. Each of these conversions has a different probability in the CG and CH contexts.
2. `fasta-polymorph` simulates a polymorphic, diploid genome based on the modified reference sequence created by `fasta-methyl-sim`.
3. `fasta-bisulf-sim` simulates reads by sampling from the simulated genome created by `fasta-polymorph`. Read are simulated with bisulfite-conversion error and sequencing error.

The reads simulated by `DNemulator` will result in $\beta$-values that have more context-dependence than those resulting from reads generated by `Sherman`. However, methylation events are still generated independently of one another, which means there is no co-methylation in the simulated data. Therefore, reads simulated by `DNemulator`, while suitable for comparing alignment strategies, are not suitable for comparing downstream analysis methods.

Other software for simulating individual bisulfite-sequencing reads are `FastqToBS` ([http://users.dimi.uniud.it/~nicola.prezza/projects.html](http://users.dimi.uniud.it/~nicola.prezza/projects.html)), which uses a similar strategy as `Sherman`, and `BSsim` ([http://122.228.158.106/BSSim/](http://122.228.158.106/BSSim/), and used in \citet{Xie:2014ez}), which has a similar strategy to `DNemulator`.

\subsection{Methods for simulating aggregate methylation levels}

The most widely studied downstream analysis problem is that of identifying differential methylation, which is done by comparing summary measures of methylation, such as $\beta$-values, between two or more groups. It is therefore generally not necessary to simulate individual reads, but is sufficient to directly simulate these aggregated measures, when simulating data for these studies.

Most papers that propose a new method for downstream analysis of bisulfite-sequencing data include a simulation section. Generally, such a simulation method exists to support claims about the performance of the proposed method and is not a major feature of the paper. Consequently, the simulation model is often only briefly described and a software implementation is rarely made available. In fact, until recently, of the methods reviewed in this section, only that of \citet{Lacey:2013iy} had a software implementation available. As I was writing this chapter, the `WGBSSuite` software was published \citep{Rackham:2015bv}. `WGBSSuite` is the only software specifically published for the purpose of simulating whole-genome bisulfite-sequencing data when comparing methods for identifying differential methylation. `WGBSSuite` is available for download as a collection of R scripts[^WGBSSuite].

[^WGBSSuite]: Unfortunately, `WGBSSuite` is not available as an R package, which is the "fundamental unit of reproducible R code" ([http://r-pkgs.had.co.nz/](http://r-pkgs.had.co.nz/)) and would greatly simplify the installation and use of the software. Furthermore, no license file is included in the download, meaning that it is unclear how the user is permitted to use `WGBSSuite` and whether they may modify or redistribute the code.

Fortunately, these simulation methods follow a common framework, even if the details differ:

0. [Optional] Simulate the locations of methylation loci.
1. Simulate the unobserved group-specific true methylation levels.
2. Simulate the observed sample-specific sequencing depths.
3. Simulate the observed sample-specific methylation levels, e.g., the $\beta$-values.

A popular choice of parametric model in this framework is the beta-binomial model \citep[e.g.,][]{Feng:2014iq, Lacey:2013iy, Xu:2013eg, Chen:2014jb, Dolzhenko:2014bo}. This model, and others, are now discussed.

\subsubsection{Simulating methylation loci}

\citet{Lacey:2013iy} and \citet{Rackham:2015bv} are notable in that they choose to simulate the locations of CpGs rather than simply using their locations in a reference genome. Both papers use hidden Markov models to simulate genomes with regions of high and low CpG density.

I do not think this a useful or necessary step and may even be counterproductive. While in truth the set of methylation loci do vary between samples due to genetic variation, it is a reasonable approximation to consider the positions of these loci as fixed. If sequence variation is required then it is easily accommodations by sampling from the set of methylation loci in the reference genome. Furthermore, the aim of the simulation model is to realistically simulate methylation _levels_, not the _locations_ of these loci.

Almost all downstream analyses are _reference-based_ (see Section \ref{sec:methylated_or_unmethylated}), so it is desirable to know how these methods perform with respect to the relevant reference genome, not some simulated genome in which the location of methylation loci vary from simulation to simulation.

\subsubsection{Simulating $B_{i, j}$}

Recall that $B_{i, j}$ is the underlying _true_ methylation level at locus $i$ in sample $j$. In the context of simulating data for comparing differential methylation calling methods, we want these to be group-specific. That is, we want to specify $B_{i, j^{k}}$, where $j^{k}$ indicates that sample $j$ comes from group $k$. For non-differentially methylated loci all $k$ groups have identical $B_{i, j^{k}} = B_{i, j^{0}}$; differentially methylated loci are simulated by setting $B_{i, j^{k}} \neq B_{i, j^{k'}}$ for some $k \neq k'$.

Under the beta-binomial model, the true methylation level, $B^{i, j}$, is assumed to follow a $Beta(\mu_{i, j^{k}}, \phi_{i, j^{k}})$ distribution, where $\mu_{i, j^{k}}$ is the mean and $\phi_{i, j^{k}}$ is the dispersion of the beta distribution[^beta_parameterisation]. Both the means, $\mu_{i, j^{k}}$, and the dispersions, $\phi_{i, j^{k}}$, are group-specific and are allowed to vary across methylation loci ($i$). The dispersion parameter models the within-group variability of the $B_{i, j}$, i.e. the within-group _biological variability_ of DNA methylation.

[^beta_parameterisation]: Note that this is different to the standard parameterisation of the Beta distribution, which is described by two shape parameters, $\alpha$ and $\beta$. The relationship between the two parameterisations is $\mu = \frac{\alpha}{\alpha + \beta}$ and $\phi = \frac{1}{\alpha + \beta + 1}$ \citep{Feng:2014iq}.

As noted by \citet{Feng:2014iq}, the beta distribution is a very flexible distribution with support on $[0, 1]$ and has "long been a natural choice to model binomial proportions", particularly as a conjugate prior, as it is used in the empirical Bayes model of \citet{Feng:2014iq}. The beta-binomial model can also be viewed from a non-Bayesian perspective as a compound distribution or overdispersed binomial distributions

Other distributions may be used instead of the beta distribution for modelling $B_{i, j}$. For example, \citet{Xie:2014ez} consider both a single Gaussian distribution and a mixture of Gaussian distributions, while \citet{Xu:2013eg} consider both a truncated Gaussian and a mixture of truncated Gaussian distributions.

An alternative to specifying a parametric distribution for the $B_{i, j}$ is to sample these from real data, e.g., by sampling some observed $\beta_{i, j}$ for a particular dataset and treating them as if they were observations on $B_{i, j}$. `WGBSSuite`\citep{Rackham:2015bv} uses a modified form of this approach. In `WGBSSuite`, a hidden Markov model is used to classify every CpG as having an underlying state ("de-methylated", "1st transition", "2nd transition" or "methylated"). Each of these four states has a region-specific average methylation level that is based on the distribution of $\beta$-values for a chosen dataset. For example, the average methylation level for all "de-methylated" regions is defined as $B_{'de-methylated'} = median(\{ \beta_{i, j}: \beta_{i, j} \in [0, 0.5) \}])$. Then, each $B_{i, j}$ is a perturbed version of this region-level average methylation, $B_{region-type}$, obtained by adding on a zero-mean Gaussian random variable, i.e. $B_{i, j} = B_{region-type} + \epsilon_{i, j}$, where $\epsilon_{i, j} \eqd Normal(0, s^{2})$. Care needs to be taken that $0 \leq B_{i, j} \leq 1$.

\subsubsection{Simulating sequence depth}

The sequencing depth at each methylation loci, $d_{i, j}$, may be sampled from real data \citep[e.g.]{Feng:2014iq, Chen:2014jb, Dolzhenko:2014bo}, or simulated from a parametric distribution such as the Poisson \citep{Rackham:2015bv}, a rounded Gaussian distribution \citep{Xu:2013eg}, or a rounded mixture of Gamma distributions \citep{Lacey:2013iy}.

The most sophisticated approach to simulation of sequencing depth in bisulfite-sequencing experiments is given by \citet{Lacey:2013iy}. In addition to using a mixture of distributions to capture both the low-coverage and high-coverage modes observed in RRBS sequencing coverage, \citet{Lacey:2013iy} model the correlation of sequencing depth across samples for a given region. They do this by using a Normal copula to make the set of sequencing depths a jointly dependent set of random variables. While this is undoubtably sophisticated, the effect of correlated versus uncorrelated sequencing depths in a simulation model is not explored in the paper and so the benefits are unclear. Moreover, it is simpler, and likely more computationally efficient, to include such correlations by a sensible sampling of sequencing depths from real data.

\subsubsection{Simulating the observed methylation levels}

The final step is to simulate the read counts, $M_{i, j}$ and $U_{i, j}$. These are based on the true underlying methylation level, $B_{i, j}$, and the sequencing depth, $d_{i, j}$. In the beta-binomial model, this is done by binomial sampling where $M_{i, j} \eqd binomial(m_{i, j} + u_{i, j}, B_{i, j})$. In addition to binomial read sampling, the `WGBSSuite` software also implements (truncated) negative binomial read sampling \citep{Rackham:2015bv}, which essentially introduces overdispersion in the read counts.

\subsubsection{Simulating differentially methylated regions}

As we have seen, the simulation of a differentially methylated locus is straightforward; for a given locus ($i$) simply vary $B_{i, j_{k}}$ across the $k$ groups. The simulation of a differentially methylated region is more complex.

In principal we can simulate a differentially methylated region by simply simulating runs of differentially methylated loci. However, it must be noted that this requires careful choice of parameters. Such parameters include the length of the DMR, the minimal number of loci it must contain, the maximal intra-pair distances of loci within the DMR and how many of the loci in the DMR must themselves be differentially methylated, e.g., should Figure \ref{fig:dmr_break} be considered one DMR or two DMRs? Such decisions ultimately have to be made by the user of the simulation software based on the types of events they are interested in.

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{../figures/dmr_break.pdf}
\caption[A hypothetical differentially methylated region]{A hypothetical region in a study of differential methylation in a two-group experiment. Plotted is the difference in $\beta$-value between the two groups ($\delta \beta$) with associated standard error against the position along the genome (\emph{Position (bp)}). The first three methylation loci are DMLs, as are the last five methylation loci. However, the fourth locus is not a DML. Should this region be considered as two distinct DMRs or as a single DMR?}
\label{fig:dmr_break}
\end{figure}

\subsection{Simulating co-methylation}

A key feature ignored by the majority of simulation methods, with the notable exceptions of \citet{Lacey:2013iy} and \citet{Rackham:2015bv}, is co-methylation. The methylation states of neighbouring loci are highly dependent and, consequently, the $B_{i, j}$ of nearby loci are highly dependent. To be clear, the majority of studies using simulated DNA methylation data have failed to model a very important feature of DNA methylation data, co-methylation.

Most simulation methods do not simulate individual reads and so cannot simulate within-fragment co-methylation. Instead, they capture correlations of methylation levels by inducing dependence in the $B_{i, j}$. For example, under the Beta-Binomial model these correlations could be induced by forcing the $\mu_{i, j}$ to be spatially dependent. This idea takes its inspiration from \citet{Jaffe:2012gx} who simulate spatially correlated DNA methylation microarray data by imposing an autocorrelation structure via a lag-1 autoregressive model of the simulated $\beta$-values.

\citet{Lacey:2013iy} take a different approach, but one that still results in correlated $B_{i, j_{k}}$ across $i$, within group $k$. To begin, they compute $\beta$-values from chromosome 11 for a single normal myotube cell line that was sequenced with RRBS. They then fit a Gaussian variogram to these $\beta$-values, which shows "a strong correlation for sites in close proximity, decaying to near independence at distances beyong 3000 bp". To simulate spatially correlated $B_{i, j}$ they use an iterative process:

1. Simulate $B_{i, j_{k}}$ from a Beta distribution with parameters estimated from the chromosome 11 MTCTL2 data. These are estimated under an assumption of independence.
2. Induce correlation amongst the $B_{i, j_{k}}$ (across $i$) by a transformation of the $B_{i, j_{k}}$.

The second step uses a method published by \citet{Zaykin:2002ko}. The transformed values, $B_{i, j_{k}}^{*}$, are created by the transformation $\mathbf{B_{j_{k}}^{*}} = 1 - \Phi{C \Phi - 1(1 - B_{j_{k}})})$, where $\mathbf{B_{j_{k}}}$ is the vector of $B_{i, j_{k}}$, $C$ is a factor of the correlation matrix $\Sigma = C C^{'}$, where $\Sigma$ is estimated from the fitted Gaussian variogram, and where $\Phi(\cdot)$ denotes the standard normal distribution function.

`WGBSSuite` induces spatial correlation amongst the $B_{i, j_{k}}$ in a less direct manner. Recall that each methylation locus is assigned one of the four underlying states ("de-methylated", "1st transition", "2nd transition", "methylated") via a hidden Markov model. The transition matrix of this hidden Markov model ensures that neighbouring loci, $i, (i+1)$, are more likely to be assigned the same state. Furthermore, since all loci within each of the four states are assigned the same (perturbed) underlying methylation level, $B_{i, j_{k}} = B_{region-type} + \epsilon_{i, j_{k}}$, neighbouring loci have similar methylation levels. Note that the $IPD$ only plays a direct role in the initial segmentation of the genome, not in the assigning of the $B_{i, j_{k}}$.

\subsubsection{Other model-based simulations}

A separate class of model-based simulation methods are those that simulate the $\beta$-values directly, i.e. without simulating sequencing depth \citet[e.g.,][]{Jaffe:2012gx, Chen:2013eh, Chen:2014jb, }. These models are designed for simulating microarray data and not sequencing data, since they do not include the variability due to variation in sequencing depth. Of these methods, only \citet{Jaffe:2012gx} simulate correlation amongst the $B_{i, j_{k}}$.

\subsection{Methods based on re-sampling real data}

A simulation may also be based entirely on re-sampling of real data[^synthetic]. This type of simulated data is attractive because through careful sampling it can capture behaviour that is otherwise very difficult, if not impossible, to capture in a parametric model. At the same time, however, if the sampling units are poorly chosen or the sampling strategy is incorrect, then it may ignore these same features or, worse still, introduce artefacts into the simulated data.

[^synthetic]: This may also be referred to as creating a synthetic dataset.

Re-sampling methods are most easily implemented at the level of $\beta$-values. Re-sampling reads is reads is more difficult, except in the special case of down-sampling whereby the positions of reads are held constant but only a sub-sample of them are used in downstream analysis. Down-sampling is only really of interest to examine the effects of sequencing coverage on downstream analyses.

\citet{Sofer:2013bk} use a re-sampling based simulation method in the development of their `Aclust` software for identifying differential methylation from microarray data. The idea is adapted from \citet{Gaile:2007bq}, which is a simulation method for array comparative genomic hybridization experiments. \citet{Sofer:2013bk} sample "blocks" of CpGs, a region of the genome where all CpGs are within 10 kb of the next, to "generate (spatial) correlation-preserved methylation data". By sampling blocks rather than individual CpGs, this sampling scheme preserves the correlation structure between CpGs occurring in the same block. Two blocks are unlikely to be correlated since there is little evidence that CpGs separated by more than 10 kb have spatially correlated methylation levels.

\citet{Sofer:2013bk} sample from a dataset of 539 Illumina 450k microarrays and select a small number of "target" CpGs, which are CpGs whose methylation level is highly variable across the 539 samples. If a block does not contain a "target" then it is sampled uniformly at random from the 539 samples. However, if a block does contain a "target" then the sampling is weighted so that the "cases" are preferentially sampled from blocks with a high level of methylation at the "target" CpG and the "controls" are preferentially sampled from blocks with a low level of methylation at the "target" CpG[^cases_and_controls]. This is essentially weighted sampling of the real data to induce differential methylation.

[^cases_and_controls]: The use of "cases" and "controls" is arbitrary, as is the choice of highly methylated for "cases" and lowly methylated for "controls" at "target" CpGs.

Due to the correlation structure of the $\beta$-values, the co-methylation, it is likely, although not guaranteed, that other CpGs in the blocks containing targets also display differential methylation.

\subsection{Summary}

My initial interest in simulation methods was to explore models of co-methylation, particularly at the level of individual DNA fragments. This requires two key capabilities:

1. Simulation of individual reads.
2. Simulation of co-methylation.

None of the published simulation methods satisfied both these requirements. This motivated the development of `methsim`.

`methsim` is specifically designed to model the co-methylation structure of bisulfite-sequencing data by simulating individual DNA fragments rather than directly simulating summary methylation measurements. In order to model the co-methylation structure, I decided that `methsim` should simulate at the level of individual DNA fragments. An added bonus of this approach is that `methsim` can (in theory) generate data at multiple resolutions: $\beta$-values, methylation patterns at m-tuples or entire sequencing reads. While my initial motivation in developing `methsim` was to explore models of co-methylation, I realised that it could also be used in the development and comparison of methods for the downstream analysis of DNA methylation data.

\section{Methods}

Simulating data with `methsim` involves 3 steps:

1. Simulating the true methylome of each sample.
2. Simulating reads, including sequencing error and bisulfite-conversion error, by sampling from the true methylome.
3. Constructing the output, be it reads, methylation patterns at m-tuples or $\beta$-values.

`methsim` requires an input dataset from which to estimate key parameters. For the input dataset, `methsim` requires the methylation patterns at various sized m-tuples, which can be produced by the `methtuple` software. `methsim` can currently only simulate CpG methylation and assumes that the methylation states of all CpGs are strand symmetric.

In what follows we will simulate data based on the _ADS_ methylome from the _Lister_ dataset. We focus on simulating autosomal data, since the sex chromosomes and mitochondrial DNA have very different methylation dynamics.

\subsection{Implementation}

`methsim` is written in R and builds on the `MethylationTuples` package described in Chapter \ref{chap:wgbs_downstream_analyses}, as well as several R packages available on Bioconductor and CRAN (see the `DESCRIPTION` file for a complete list.). It is currently a very experimental package and is therefore not yet published on Biocondutor, but its development can be followed at [https://github.com/PeteHaitch/methsim](https://github.com/PeteHaitch/methsim).

`methsim` makes extensive use of the S4 object system in R. The most important classes defined by `methsim` are `MethylomeParam`, `SimulatedMethylome`, `WGBSParam` and `SimulatedBS`. The most important methods are the `simulate()` methods defined for the `MethylomeParam` and `WGBSParam` classes.

The `MethylomeParam` object contains the empirical distributions of key statistics from which the parameters for simulating the 'true' methylome are sampled. Therefore, the `MethylomeParam` should be based on data from a relevant sample. To help a new user get started, `methsim` includes `MethylomeParam` objects for the _ADS_, _ADS-adipose_ and _ADS-iPSC_ samples from the _Lister_ dataset. Alternatively, the user may process the `BAM` file for their sample with `methtuple` and then use the helper functions in `methsim` to construct a `MethylomeParam` object based on their sample of interest.

To simulate a true methylome, the user runs the `simulate()` method on the `MethylomeParam` object. This returns a `SimulatedMethylome` object.

Once we have a true methylome, we can simulate data from it. `methsim` currently supports the simulation of whole-genome bisulfite-sequencing reads via the `WGBSParam` class and associated `simulate()` method. Other assays, such as RRBS or microarrays could in principle be supported. A `WGBSParam` object will contain a `SimulatedMethylome` object, along with parameters such as the read-length, sequencing coverage and error rate of the data to be simulated. When applied to a `WGBSParam` object, the `simulate()` method returns a `SimulatedBS` object or a `MethylationTuples::MethPat` object[^namespace2]. A `SimulatedBS` object contains all simulated reads and is generally a large object (on the order of $10$ GB). By contrast, the `MethylationTuples::MethPat` object, which summarises the simulated reads for m-tuples of a particular size[^methpat], is much smaller (on the order of $100$ MB) but does not contain the full information of the simulation since read-level data are lost. `methsim` also includes a helper function, `asMethPat()`, to coerce a `SimulatedBS` object to a `MethylationTuples::MethPat` object.

[^namespace2]: This uses the NAMESPACE notation of R: `MethylationTuples::MethPat` can be read as "the `MethPat` class is part of the `MethylationTuples` package". See Section \ref{sec:MethylationTuples} for details of this class.

[^methpat]: Returning a `MethylationTuples::MethPat` object may be appropriate, for example, if all that is required for downstream analyses are methylation counts at 1-tuples.

\subsection{Simulating a single methylome}

As we have seen, DNA methylation is highly heterogeneous along the genome. Nonetheless, there are clear regions of "similarity", such as the unmethylated CpG islands and long partially methylated domains. In these locally-similar regions we might hope to model DNA methylation by a simple parametric model. `methsim`, like other simulation methods, is based on the idea of segmenting the genome into 'regions of similarity', fitting a simple model to each region and then 'stitching' the results together to form the true methylome.

The idea of segmenting a globally heterogenous stochastic process into a series of locally homogeneous processes is not new. A hidden Markov model is an example of such a process; while the entire process may highly heterogeneous, conditional on the hidden states the process may be homogeneous. Hidden Markov models, and other models assuming local similarity in spite of global heterogeneity, have been used with great success in bioinformatics.

`methsim` takes the following approach to simulating the true underlying methylome for each sample:

1. Segment the methylome into regions of similarity.
2. Sample parameters for each methylation locus, $i$, based on the segmentation.

I now describe each step in greater detail.

\subsubsection{Segmenting the methylome}

`methsim` uses the the R/Bioconductor package, `MethylSeekR` \citep{Burger:2013kq}, to segment the input methylome into regions of similarity. `MethylSeekR` was developed to discover regulatory motifs from bisulfite-sequencing data by segmenting the methylome into unmethylated regions (\emph{UMR}s), lowly-methylated regions (\emph{LMR}s) and partially methylated regions[^pmds] (\emph{PMR}s). It does this using a two-stage algorithm applied to the $\beta$-values from a sample:

[^pmds]: Partially methylated regions are also commonly known as partially methylated domains, but we will refer to them as 'regions' for consistency with UMRs and LMRs.

1. Identify partially methylated regions. A summary statistic, $\alpha$, which is based on the $\beta$-values in a sliding window of 100 CpGs, is used to identify PMRs. Briefly, a two-state hidden Markov model is fit to the $\alpha$ values to identify PMRs and non-PMRs.
2. Identify UMRs and LMRs. The PMRs are masked from the genome and simple heuristics are used to identify UMRs and LMRs based on the average $\beta$-values in a window and the number of CpGs in the window.

`methsim` post-processes the segmentation provided by `MethylSeekR` to partition the methylome into regions of similarity, namely \emph{UMR}s, \emph{LMR}s, \emph{PMRS}s and \emph{other}s[^methylseekr_output]. Roughly speaking, _other_ regions are 'mostly methylated regions' (see Figures \ref{fig:EPISCOPE_beta_by_pm_boxplots}, \ref{fig:Lister_beta_by_pm_boxplots}, \ref{fig:Seisenberger_beta_by_pm_boxplots}, \ref{fig:Ziller_beta_by_pm_boxplots}), although \citet{Burger:2013kq} do not describe these regions as such.

[^methylseekr_output]: The output returned by `MethylSeekR` does not strictly partition the methylome since it is neither disjoint nor exhaustive.

While a tailored algorithm may improve this segmentation process, the result produced by `MethylSeekR` is a reasonable approximation to segmenting the methylome into the required 'regions of similarity'.

\subsubsection{Parameterising the model}\label{subsec:parameterising_the_model}

`methsim` uses a two-state ($1 =$ methylated or $0 =$ unmethylated), one-step Markov chain to model $\mathbf{Z} = \{ \mathbf{Z_{h}} \}_{h = 1}^{h = n_{h}}$. This allows the incorporation of within-fragment co-methylation into the simulation. The choice of a one-step process is one of computational simplicity and is a reasonable approximation for most of the genom given the available data (see Chapter \ref{chap:co-methylation}).

In Chapter \ref{chap:co-methylation} we saw that the strength of within-fragment co-methylation varies as a function of the intra-pair distance and by the genomic context. For this reason I allow the transition probabilities, $\mathbf{p}$, to vary with $i$, that is, I allow the Markov chain to be spatially inhomogeneous. In particular, I allow the transition probabilities to depend on the intra-pair distance between the $i^{th}$ and $(i + 1)^{th}$ locus and on the region type, $r_{i}$ (_UMR_, _LMR_, _PMR_ or _other_), for this pair of loci[^context]. That is, $p_{i; a, b} = Pr(Z_{h, i + 1} = b | Z_{h, i} = a, IPD(i, i + 1), r_{i})$.

[^context]: Actually, it only depends on the region of the $i^{th}$ locus. Most pairs of loci, $(i, i + 1)$, will lie in the same region. For pairs that span the boundary of two different regions I have arbitrarily chosen to use the region of the $i^{th}$ locus.

The above-described model is not particularly amenable to analytical calculations due to the spatial inhomogeneity. It is, however, relatively simple to simulate realisations from this model, requiring only a single loop over the set of loci on each chromosome. For a chromosome containing $n$ methylation loci, there are $n - 1$ transition matrices to estimate or otherwise assign.

Rather than directly modelling the transition probabilities, $\mathbf{p}$, `methsim` is parameterised by a vector of marginal probabilities, $\mathbf{B} = \{ Pr(Z_{i} = 1) \}$, and a vector of odds ratios, $\mathbf{\psi} = \{ \frac{Pr_{Z_{i + 1} = 1 | Z_{i} = 1} \times Pr_{Z_{i + 1} = 0 | Z_{i} = 0}}{Pr_{Z_{i + 1} = 1 | Z_{i} = 0} \times Pr_{Z_{i + 1} = 0 | Z_{i} = 1}} \} = \{ \frac{Pr_{Z_{i} = 1, Z_{i + 1} = 1} \times Pr_{Z_{i} = 0 , Z_{i + 1} = 0}}{Pr_{Z_{i} = 1 , Z_{i + 1} = 0} \times Pr_{Z_{i} = 0 , Z_{i + 1} = 1}} \}$. We can compute the transition probabilities, $\mathbf{p}$, from $\mathbf{B}$ and $\mathbf{\psi}$.

For each 2-tuple, $(i, i + 1)$, `methsim` first constructs the joint probability matrix, $P_{i, i + 1}$ from the marginal probabilities, $B_{i}, B_{i + 1}$, and the odds ratio, $\psi_{i}$. The general form of $P_{i, i + 1}$ is shown below:

\begin{equation*}
P_{i, i + 1} = \bordermatrix{~ & 1 - B_{i+1} & B_{i+1} \cr
1 - B_{i} & Pr(Z_{i} = 0, Z_{i + 1} = 0) & Pr(Z_{i} = 0, Z_{i + 1} = 1) \cr
B_{i} & Pr(Z_{i} = 1, Z_{i + 1} = 0) & Pr(Z_{i} = 1, Z_{i + 1} = 1) \cr}
\end{equation*}

`methsim` computes $P_{i, i + 1}$ using the iterative proportional fitting algorithm. Iterative proportional fitting is a general method "for constructing tables of numbers satisfying certain constraints" \citep{Speed:2005tg}. In the case of `methsim`, we use iterative proportional fitting to construct the _unique_ $2 \times 2$ array (the joint probability matrix) with specified margins (the marginal probabilities of each methylation state at each locus) and the specified cross-ratio (the odds-ratio). An example of this procedure is illustrative.

Let $B_{i} = 0.7$, $B_{i + 1} = 0.6$ and $\psi_{i, i + 1} = 2$. To begin the iterative proportional fitting algorithm, form the matrix $P_{i, i + 1}^{(0)} = \Bigl(\begin{smallmatrix}
\psi & 1 \\
   1 & 1
\end{smallmatrix} \Bigr)$. This matrix has the desired cross-ratio, $\psi$, but not the desired row and column margins. At each iteration of the algorithm, iterative proportional fitting adjusts the rows and columns such that the cross-ratio remains $\psi$ while the row and column margins converge towards the desired values. The algorithm continues in this manner, forming a series of $2 \times 2$ tables that converge pointwise (and uniquely) to a $2 \times 2$ table $P_{i, i + 1}^{(*)} \equiv P_{i, i + 1}$. For our example, $P_{i, i + 1}^{(*)} = \Bigl(\begin{smallmatrix}
0.155 & 0.145 \\
0.245 & 0.455
\end{smallmatrix} \Bigr)$, to three decimal places of precision. It is easily verified that $P^{(*)}$ has the desired cross-ratio and marginal sums.

Once $P_{i, i + 1}$ is computed, the desired transition probability is obtained by dividing the appropriate element of $P_{i, i + 1}$ by the appropriate marginal probability. To continue our example, suppose $Z_{i} = 0$, then the probability that $Z_{i + 1}$ is also zero is given by $Pr(Z_{i + 1} = 0 | Z_{i} = 0) = \frac{Pr(Z_{i = 0}, Z_{i + 1} = 0)}{Pr(Z_{i} = 0)} = \frac{0.155}{0.3} = 0.517$ (to three decimal places). In fact, `methsim` only stores $Pr(Z_{i + 1} = 1 | Z_{i} = 0)$ and $Pr(Z_{i + 1} = 1 | Z_{i} = 1)$ since $Pr(Z_{i + 1} = 0 | Z_{i} = 0) = 1 - Pr(Z_{i + 1} = 1 | Z_{i} = 0)$ and $Pr(Z_{i + 1} = 0 | Z_{i} = 1) = 1 - Pr(Z_{i + 1} = 1 | Z_{i} = 1)$.

The above-described model implicitly assumes that all $n_{h}$ haplotypes have the same marginal probabilities and co-methylation structure. We may extend this model to define $\mathbf{Z}$ as a _mixture_ of a small number of first-order Markov chains defined as in the above. Such a mixture model may be appropriate when simulating a sample that is a combination of cell types.

To simulate values of $\mathbf{B}$ and $\mathbf{\psi}$, `methsim` uses the empirical distributions of their estimates, $\mathbf{\beta}$ and $\widehat{\mathbf{\psi}}$, in the input methylome. More specifically, $\mathbf{B}$ is based on $\beta_{i | r_{i}}$ (the empirical distribution of $\beta_{i}$ conditional on the region type of the $i^{th}$ locus, $r_{i}$) and $\mathbf{\psi}$ is based on $\widehat{\psi}_{MH}$. We have already seen many examples of the distribution of $\widehat{\psi}_{MH}$ in Chapter \ref{chap:co-methylation}. The empirical distributions $\beta_{i | r_{i}}$ for the _ADS_ sample are shown in Figure \ref{fig:ADS_beta_by_pm_boxplots}.

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{../figures/ADS_beta_by_pm_boxplots.pdf}
\caption[Distribution of $\beta$-values stratified by region type]{$\beta_{i | r_{i}}$, the distribution of $\beta$-values for CpGs in each region type, for the \emph{ADS} sample. Only CpGs with at least $10\times$ sequencing coverage are used. UMR = unmethylated region; LMR = lowly methylated region; PMR = partially methylated region; other = any other region.}
\label{fig:ADS_beta_by_pm_boxplots}
\end{figure}

There are many ways to simulate values of $\mathbf{B}$ and $\mathbf{\theta} = \log{\mathbf{\psi}}$ based on these empirical distributions. I have so far only had time to explore a few simple methods and this remains ongoing work. We will discuss the results from three methods for simulating $\mathbf{B}$ (_m1_, _m2_, and _m3_) and two ways for simulating $\mathbf{\theta}$ (_A_ and _B_):

\begin{description}
  \item[\emph{m1}] All loci in each region have the same $B$, which is sampled from $\beta_{i | r_{i}}$.
  \item[\emph{m2}] All loci in each region have the same \textbf{average} $B$, which is sampled from $\beta_{i | r_{i}}$; this is then perturbed by a $Gaussian(0, \sigma_{B}^2)$ random variable. This is similar to what is done by `WGBSsuite`. Here we use $\sigma_{B}^{2} = B \times (1 - B)$. The resulting $\mathbf{B}$ is truncated so that all values lie between 0.01 and 0.99 (necessary to avoid issues in the iterative proportional fitting algorithm at the boundaries).
  %\item[\emph{m3}] $\mathbf{B}$ simulated as in \emph{m2} but with the perturbations in each region simulated from a first-order autoregressive process with coefficient equal to 0.5.
  %\item[\emph{A}] $\theta$ identically zero. This is to simulate independence of within-fragment methylation states and as a control to check that `methsim` is working as intended for simulating within-fragment co-methylation.
  %\item[\emph{B}] $\theta$ sampled from $Gaussian(\mu_{IPD}, \sigma_{IPD}^{2})$, where $\mu_{IPD}$ and $\sigma_{IPD}$ are plug-in estimates computed from the distribution of chromosome-level $\widehat{\theta_{MH}}$ (autosomes only).
\end{description}

Furthermore, we will consider simulations where the sample is 'pure' (\emph{I}) and where the sample is a mixture of sub-populations with relative frequencies __TODO: Check frequencies__ (\emph{II}). We do not explore all combinations of these factors but look at five informative combinations: _m1AI_, _m1BI, _m2BI_, _m3BI_ and _m3BII_. We compare these five models to the real _ADS_ data.

The simulation of $\mathbf{\theta} = \log{\mathbf{\psi}}$ requires further explanation. The empirical distributions of $\widehat{\mathbf{\theta}}$ will only include values for small $IPD$s since these are estimated from individual sequencing reads (see Chapter \ref{chap:co-methylation} for details). When simulating a methylome, we will encounter 2-tuples with much larger $IPD$s and the question is how to deal with these?

Again, there are several options. For example, we might assume a parametric form for the distribution of $\mathbf{\psi}$ as a function of $IPD$, such as a exponential decay towards $\theta = 0$ (corresponding to independence). Given the quality and read-lengths of the _ADS_ data, I have elected to use the empirical $\widehat{\theta}_{MH}$ for pairs with $IPD \leq 180$ and set $\mu_{IPD} = 0$ for those with $IPD > 180$. $\sigma_{IPD}$ is given by the median absolute deviation of $\sigma_{IPD}$ for $IPD \leq 180$. While obviously a gross simplification, the vast majority of $NIL = 0$ CpG pairs have an $IPD \leq 180$ and so will be unaffected by this 'independence' assumption.

There is an increase in complexity for models m1 through to m4, while m5 is there as a sanity check that the within-fragment co-methylation model is working as intended. The limitations of these models may already be clear, but we will discuss these further in Section \ref{sec:methsim_results}.

\subsection{Simulating reads}

Once we have our 'true' methylome, we want to simulate an assay of this sample. In theory this could be any type of methylation assay, but here we focus on simulating whole-genome bisulfite-sequencing data.

`methsim` uses a simple Poisson-based method for simulating bisulfite-sequencing. The user specifies the desired read length, the average sequencing coverage[^read_length], the error rate ($\epsilon$, which includes both sequencing error and bisulfite-conversion error), and provides the simulated 'true' methylome. We will use $200$ bp, single-end reads with an average coverage of $30 \times$ in what follows.

[^read_length]: Currently only single-end data are supported. This isn't a big issue. Most paired-end bisulfite-sequencing datasets have overlapping mates, and so paired-end data can be approximated by simply doubling the read length. Also, all reads must currently have the same length, i.e. no simulation of read trimming. I don't consider this feature a priority, but it could easily be implemented by generating read lengths from a given probability distribution.

The number of reads required is computed by $n_{reads} = \frac{\text{average sequencing coverage}}{\text{read length}} \times \text{size of genome}$. The number of reads per chromosome is assigned proportional to the chromosome length. Then, the start of each read is sampled uniformly across the chromosome respective chromosome. We only retain those reads that overlap a methylation locus.

Suppose we have a simulated read, $z$, that overlaps the 3-tuple $(i, i + 1, i + 2)$. The methylation state along the read is simulated as follows:

1. Sample $z_{i}$ from a Bernoulli($B_{i}$) distribution[^mixture].
2. Sample $z_{i + 1}$ from a Bernoulli($p_{i}$) distribution, where $p_{i}$ is the appropriate transition probability given the simulated $z_{i}$.
3. Sample $z_{i + 2}$ from a Bernoulli($p_{i + 1}$) distribution, where $p_{i + 1}$ is the appropriate transition probability given the simulated $z_{i + 1}$.
4. Each of element of the simulated $z$ is independently flipped with probability $\epsilon$ to simulate sequencing error.

[^mixture]: If $\mathbf{Z}$ is a mixture of Markov chains then we first simulate which component the read comes from by sampling from a multinomial($w$) distribution, where $w$ is the vector of weights of each component. All subsequent steps will be simulated according to which component is sampled at this step.

The result of this process is a `SimulatedBS` object storing the ID of each read that overlaps a methylation locus, along with the corresponding genomic co-ordinates and the methylation states.

\subsection{Constructing the output}

The `SimulatedWGBS` object contains all the data from the simulation. However, it is not always the most convenient or efficient format with which to work. For example, many downstream analyses only make use of $\beta$-values, so we might want to summarise our simulated data this way. `methsim` provides the `asMethPat()` function to convert the `SimulatedWGBS` object to a `MethylationTuples::MethPat` object containing methylation patterns at m-tuples of a given size[^simplify]; all the functionality of the `MethylationTuples` package is then available to the user, such as computing $\beta$-values with the `methLevel()` method.

[^simplify]: Alternatively, the user may use the `simplify` argument of the `simulate()` method to return an already 'simplified' `MethylationTuples::MethPat` object rather than the `SimulatedWGBS` object.

\subsection{Simulating multiple samples}

The above describes how to simulate a single whole-genome bisulfite-sequencing sample. If we wanted to simulate six independent samples then we could simply run this procedure six times, with a different `MethylomeParam` object for each realisation. However, we will generally be interested in simulating experiments where there is some relationship between the samples. For example, suppose we want to simulate a two-group experiment with three samples per group. Furthermore, suppose we want to include differentially methylated regions between the two groups. How do we simulate such an experiment?

To simulate these type of experiments, we need to be able to control the variation between the resulting `SimulatedWGBS` objects. In this example, we probably want the three samples in each group to be more similar to each other than to the three samples in the other group. Essentially, we need to be able to control the within-group and between-group variation.

Biological variation is controlled through the `MethylomeParam` object and the method with which we simulate $\mathbf{B}$ and $\mathbf{\psi}$ through the `simulate()` method. Technical variation is controlled the `WGBSParam` object. If we want two samples to be very different from one another, then we would use two different `MethylomeParam` objects at the beginning of the process. However, if we want to simulate two technical replicates, then we would use the same `WGBSParam` objects; the two samples have identical 'true' methylomes and they only diverge at the 'sequencing' step of the simulation.

We might also consider more subtle ways of introducing variation. For example, we might use the same `MethylomeParam` object but vary how we simulate $\mathbf{B}$ and $\mathbf{\psi}$ between calls to `simulate()`. This avenue remains to be explored.

\subsubsection{Simulating differential methylation}\label{sec:sim_diff_meth}

An obvious application of `methsim` is to simulate data to be used in a study comparing methods for identifying differential methylation.

One way of doing this would be to take a `SimulatedMethylome` object and create a modified copy where $\mathbf{B}$ has been perturbed by specified amounts at a set of specified loci. We could then simulate bisulfite-sequencing data from each `SimulatedMethylome` and study which analysis methods can identify these simulated DMCs and DMRs.

\subsubsection{Performance}

Much of `methsim` is written in C++, using the wonderful Rcpp package \citep{Eddelbuettel:2011to, Eddelbuettel:2013if}, which greatly speeds up the running time of key procedures. Furthermore, several steps of the simulation can be run in parallel for each chromosome. Parallelism is implemented via the `BiocParallel` Bioconductor package \citep{BiocParallel}.

The `SimulatedMethylome` and `SimulatedBS` objects are large in memory. This is to some extent unavoidable; we are simulating whole-genome bisulfite-sequencing sequencing experiments that produce an enormous amount of data.

Using up to 8 CPU cores in parallel, it takes less than 9 minutes to simulate a single $30 \times$ sequencing coverage whole-genome bisulfite-sequencing assay from a 'true' methylome. These results are indicative of simulating high-coverage whole-genome bisulfite-sequencing for human-sized genomes. Simulations using a smaller genome or lower sequencing coverage will have faster running times and lower memory usage.

\section{Results}\label{sec:methsim_results}

A central feature of `methsim` is that the simulation parameters are sampled from an input sample. At a bare minimum, the first test of `methsim` is that simulated data are similar to the real data on which they are based. To assess this, we compare several summary measures between the simulated data and the real data. The simplest of these are summaries of the distributions of $\beta$-values and within-fragment co-methylation, which are explicitly sampled in `methsim`.

First looking at the distribution of $\beta$-values, we see that all of _m1AI_, _m1BI_, _m2BI_, _m3BI_ and _m3BII_ do a reasonable job of capturing the bimodality of $\beta$-values (Figure \ref{fig:methsim_plotBetaFreqPoly}) and their relationship to CpG islands (Figure \ref{fig:methsim_plotBetaFreqPoly_CGI}). The effect of both the independent perturbations (_m2_) and the correlated perturbations (_m3_) appear to be dominated by variation due to variation in sequencing coverage. The lack of within-fragment co-methylation in _m1AI_ does not affect the genome-wide distribution of methylation levels. The mixture model, _m3BII_, has noticeably more intermediate $\beta$-values owing to the increased heterogeneity at each CpG across the sub-populations.

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{../figures/methsim_plotBetaFreqPoly.pdf}
\caption[\texttt{methsim} $\beta$-value frequency polygon]{Frequency polygon of the genome-wide distribution of CpG $\beta$-values for \texttt{methsim} simulated data and the \emph{ADS} input sample. $\beta$-values are grouped into $0.01$-width bins and the percentage of CpGs in each bin is plotted on the y-axis. Observations have been combined across strands and only CpGs with at least $10 \times$ sequencing coverage are included.}
\label{fig:methsim_plotBetaFreqPoly}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{../figures/methsim_plotBetaFreqPoly_CGI.pdf}
\caption[\texttt{methsim} $\beta$-value kernel density stratified by CpG islands]{Kernel density estimates of the genome-wide distribution of CpG $\beta$-values for the \texttt{methsim} simulated data and the \emph{ADS} input sample, stratified by whether the CpG is in a CpG island. Only CpGs with at least $10 \times$ sequencing coverage are included. `Spikes' in the density estimate are due to the discreteness of $\beta$-values.
\label{fig:methsim_plotBetaFreqPoly_CGI}
\end{figure}

Turning our attention to within-fragment co-methylation, Figure \ref{fig:methsim_methsim_MH_by_seqnames} shows the genome-level and chromosome-level Mantel-Haenszel estimates of within-fragment co-methylation. Figure \ref{fig:methsim_MH_by_CGI} shows the genome-level estimates of within-fragment co-methylation stratified by CpG islands. Reassuringly, we see no evidence of within-fragment co-methylation in _m1AI_, which simulates independence of within-fragment methylation states. The trend of the genome-level estimates, by design, very closely matches that of the _ADS_ input sample. We see, however, that the chromosome-level estimates of _m1BI_, _m2BI_, _m3BI_ and _m3BII_ are perhaps a little too homogeneous when compared to the _ADS_ input sample. We also see in the results for _m1BI_, _m2BI_, _m3BI_ and _m3BII_ the effect of simulating from a $Gaussian(0, \sigma)$ for $IPD > 180$, as outlined in Section \ref{subsec:parameterising_the_model}. The _m3BII_ model again stands out, this time with a noticeably higher level of within-fragment co-methylation. Each sub-population has its own $\mathbf{\theta}$ and, although these are sampled from the same parametric model, this suggests that such heterogeneity in the sample will artificially inflate estimates of within-fragment co-methylation, even those made at the genome-level. While this requires further investigation, it raises the possibilit that increased genome-level estimates of within-fragment co-methylation may in fact be measuring increased heterogeneity of the sample.

These caveats aside, all five models do quite a good job of capturing the average level of CpG methylation and the strength of within-fragment co-methylation. Unfortunately, the results are not so promising for the correlations of $\beta$-values.

In contrast to the other two summaries, the intention with `methsim` was to not explicitly model the spatial correlation of $\beta$-values. Rather, the idea was to see how the marginal level of methylation and the within-fragment co-methylation affected these correlations[^ar]. However, this overlooks that we are already imposing some structure, and therefore correlations, on the $\textbf{B}$  by specifying them in a region-specific manner.

[^ar]: While _m3BI_ and _m3BII_ do include some spatial dependence of the $\mathbf{B}$ via the autoregressive perturbation, these perturbations are independent of $IPD$ and are really intended to analyse the effect of the independence condition in _m2BI_.

Figures \ref{methsim_spearman_beta_cors_min_cov_10_strand_collapsed_CI.pdf} shows the overwhelming nature of this modelling decision. When compared to the correlations of $\beta$-values from the _ADS_ input sample, we see that in all five models the correlations of $\beta$-values are far too strong. Stratifying these correlations by CpG islands, we see that the source of this problem is in regions outside of CpG islands (Figure \ref{methsim_spearman_beta_cors_min_cov_10_strand_collapsed_CGI.pdf}). This gives some hope that this deficiency may fixed by a more careful modelling of $\beta$-values outside CpG islands, and remains a source of ongoing work.

\section{Summary}

Simulating DNA methylation data is complicated by its heterogeneity. This heterogeneity exists is in multiple 'directions': along the genome, between cells in a sample, and between subjects. As more data become available, we can get a better handle on the causes of the variation but, for now, it remains challenging.

`methsim` provides a framework that I have used to experiment with models for simulating DNA methylation data. The models I have currently explored capture some key aspects of the data including, for the first time, within-fragment co-methylation. Unfortunately, while these results are promising on some fronts, there is clearly much work to be done on others.

Most notably, and frustratingly, I do not yet have an adequate model for the correlations of the true methylation levels, $\mathbf{B}$. The model of \citet{Lacey:2013iy} is promising, although may be computationally infeasible for whole-genome data given the timings reported in the original publication. The model used by `WGBSsuite` \citet{Rackham:2015bv} is similar to m2, so I suspect it will produce similar results, but this requires further investigation. I continue to explore how alternative models of $\mathbf{B}$ might be integrated into `methsim` to improve this aspect of the simulation.
